// Licensed under the MIT License. See LICENSE file in the project root for details.

//go:build amd64 && !purego && amd64.v4

// This file implements bytes-equality using AVX2 for Go on amd64.
// It compares two byte slices a and b and returns true if all bytes match.
//
// A few quick primers for readers new to Go assembly and SIMD:
// - Go assembly uses Plan 9 syntax: MOVQ src, dst (source first, destination second).
// - Function parameters are accessed via stack frame pseudo-register FP with offsets
//   generated by the Go toolchain. For a slice parameter `[]byte`, the layout is:
//     base pointer at offset +0, length at +8, capacity at +16. For the second slice,
//     offsets continue: base at +24, length +32, capacity +40, etc.
// - General purpose registers we use here include SI/DI (often used as source/dest
//   pointers), CX/DX (counters/lengths), AX (accumulator), R8 (scratch), etc.
// - AVX2 YMM registers (Y0..Y15) are 256-bit wide, so each can hold 32 bytes.
// - We compare 32-byte blocks with AVX2, then handle a possible 16-byte tail with SSE,
//   then any final remaining bytes with a scalar byte-compare loop.

#include "textflag.h"

// avx2ChunkSize defines the number of bytes processed per AVX2 iteration.
// A dedicated constant avoids magic numbers and eases future adjustments.
// We process 32 bytes per AVX2 iteration (one YMM register).
// avx2ChunkShift is log2(32) = 5, used to compute block counts by shifting.
// avx2ChunkMask (31) is used to compute the remainder (len % 32).
#define avx2ChunkSize 32
#define avx2ChunkShift 5
#define avx2ChunkMask 31

// sseChunkSize defines bytes processed per SSE iteration when handling tails.
// SSE XMM registers are 128-bit = 16 bytes. We use these for a 16-byte tail.
// sseChunkShift is log2(16) = 4. sseChunkMask (15) computes len % 16.
#define sseChunkSize 16
#define sseChunkShift 4
#define sseChunkMask 15

// bytesEqualAVX2 compares two byte slices for equality using AVX2 instructions.
// It safely handles misaligned tails by falling back to SSE and scalar code paths.
// func bytesEqualAVX2(a, b []byte) bool
TEXT Â·bytesEqualAVX2(SB), NOSPLIT, $0-49
        MOVQ a_base+0(FP), SI    // Load a's data pointer into SI (source index)
        MOVQ a_len+8(FP), CX     // Load len(a) into CX (counter register)
        MOVQ b_base+24(FP), DI   // Load b's data pointer into DI (dest index)
        MOVQ b_len+32(FP), DX    // Load len(b) into DX

        // Lengths must match; if they differ, slices cannot be equal
        CMPQ CX, DX              // Compare len(a) vs len(b)
        JNE  not_equal           // If not equal, return false immediately
        // If length is zero, both are empty; they are equal
        TESTQ CX, CX             // Set flags based on CX (len == 0?)
        JE   equal               // Jump to equal if zero-length

        MOVQ CX, R8              // Copy total length to R8 (won't destroy CX yet)
        SHRQ $avx2ChunkShift, R8 // R8 = number of 32-byte AVX2 blocks (len / 32)
        ANDQ $avx2ChunkMask, CX  // CX = remainder bytes after AVX2 (len % 32)

        TESTQ R8, R8             // Are there any full 32-byte blocks to compare?
        JE   avx2_tail           // If none, skip AVX2 loop and handle tail

avx2_loop:
        VMOVDQU (SI), Y0        // Load 32 bytes from a[SI..SI+31] into Y0 (unaligned ok)
        VMOVDQU (DI), Y1        // Load 32 bytes from b[DI..DI+31] into Y1
        VPCMPEQB Y1, Y0, Y2     // Per-byte compare: Y2 lanes = 0xFF if equal, 0x00 if not
        VPMOVMSKB Y2, AX        // Extract the high bits of each byte to a 32-bit mask
        CMPL    AX, $0xFFFFFFFF // If all bytes equal, mask is all 1s (0xFFFFFFFF)
        JNE     not_equal       // Any mismatch -> return false
        ADDQ    $avx2ChunkSize, SI // Advance a pointer by 32 bytes
        ADDQ    $avx2ChunkSize, DI // Advance b pointer by 32 bytes
        DECQ    R8              // Decrement remaining AVX2 block count
        JNZ     avx2_loop       // Continue loop until all blocks processed

avx2_tail:
        // After AVX2 blocks, check if there is a 16-byte chunk to handle with SSE
        CMPQ CX, $sseChunkSize  // Do we have at least 16 bytes remaining?
        JB   scalar_tail        // If fewer than 16, go to scalar tail
        MOVOU  (SI), X0         // Load 16 bytes from a into XMM0 (unaligned ok)
        MOVOU  (DI), X1         // Load 16 bytes from b into XMM1
        PCMPEQB X1, X0          // Per-byte compare: X0 lanes = 0xFF if equal, else 0x00
        PMOVMSKB X0, AX         // Compress the high bits of X0 bytes into 16-bit mask
        CMPL   AX, $0xFFFF      // All bits set means all 16 bytes matched
        JNE    not_equal        // If any bit is zero, there was a mismatch
        ADDQ   $sseChunkSize, SI // Advance pointers by 16 bytes
        ADDQ   $sseChunkSize, DI
        SUBQ   $sseChunkSize, CX // Reduce remainder by 16

scalar_tail:
        TESTQ CX, CX            // Any bytes left to compare?
        JE    equal             // If no remainder, slices are equal
tail_loop:
        MOVB  (SI), AL          // Load next byte from a
        MOVB  (DI), DL          // Load next byte from b
        CMPB  AL, DL            // Compare bytes
        JNE   not_equal         // Any mismatch -> not equal
        INCQ  SI                // Advance pointers
        INCQ  DI
        DECQ  CX                // Decrement remaining count
        JNZ   tail_loop         // Loop until no remainder
        JMP   equal             // All remaining bytes matched

equal:
        MOVB $1, ret+48(FP)     // Store true (1) to the return slot
        VZEROUPPER              // Clear upper YMM state to avoid AVX-SSE penalties
        RET                     // Return to caller

not_equal:
        MOVB $0, ret+48(FP)     // Store false (0) to the return slot
        VZEROUPPER              // Clear upper YMM state for cleanliness/performance
        RET                     // Return to caller
